{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:47:09.352691Z",
     "start_time": "2024-10-03T17:47:09.330223Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DigiSchool/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a451bd62023ab8f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:48:43.749952Z",
     "start_time": "2024-10-03T17:48:43.703566Z"
    }
   },
   "outputs": [],
   "source": [
    "# env\n",
    "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyC6A1MJR-kk-KetpF3Llqna_GE4hulhwMU\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash', temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ad3accef0742a47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:48:50.856969Z",
     "start_time": "2024-10-03T17:48:49.761510Z"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import zipfile\n",
    "\n",
    "import requests\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "data_url = \"https://storage.googleapis.com/benchmarks-artifacts/langchain-docs-benchmarking/cj.zip\"\n",
    "result = requests.get(data_url)\n",
    "filename = \"cj.zip\"\n",
    "with open(filename, \"wb\") as file:\n",
    "    file.write(result.content)\n",
    "\n",
    "with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "655ac87483d854e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:48:55.925216Z",
     "start_time": "2024-10-03T17:48:55.497396Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"./cj/cj.pdf\")\n",
    "docs = loader.load()\n",
    "tables = []\n",
    "texts = [d.page_content for d in docs]\n",
    "full_document = \" \".join(texts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba2e41f9d949408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:49:02.675259Z",
     "start_time": "2024-10-03T17:48:59.163013Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/DigiSchool/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1727982304.633933 1667825 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "/var/folders/1v/615z7l0x089_234ybn3s708w0000gn/T/ipykernel_30728/986609046.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be9dc973562bd238",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:28:04.210854Z",
     "start_time": "2024-10-03T17:28:04.206246Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split the full document into smaller chunks using RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_text(full_document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67ae35d0e7469750",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:28:06.293759Z",
     "start_time": "2024-10-03T17:28:06.290214Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create document objects from the splits\n",
    "split_docs = [Document(page_content=chunk) for chunk in splits]  # Ensure each chunk is a Document object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3707eb4d75b22754",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:28:09.665321Z",
     "start_time": "2024-10-03T17:28:08.575950Z"
    }
   },
   "outputs": [],
   "source": [
    "# Embed the document chunks using the embedding model and store them in a vectorstore\n",
    "vectorstore = Chroma.from_documents(documents=split_docs, embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f457a8da7890212",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:34:29.831080Z",
     "start_time": "2024-10-03T17:34:29.203088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "prompt_template = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a3f3525b8fca3fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T19:04:01.598129Z",
     "start_time": "2024-10-03T19:04:01.519946Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f339d15b7a2c5c20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:34:30.756274Z",
     "start_time": "2024-10-03T17:34:30.752632Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a502e3e1bb0446e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:34:34.270746Z",
     "start_time": "2024-10-03T17:34:34.245320Z"
    }
   },
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt_template\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14c2ef6e3bdfc1be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:34:35.621261Z",
     "start_time": "2024-10-03T17:34:35.599976Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1727978062.850702 1576979 fork_posix.cc:77] Other threads are currently calling into gRPC, skipping fork() handlers\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"help me summarize this\"\n",
    "\n",
    "res = rag_chain.invoke(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "beb70addfbd64e3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:34:39.335559Z",
     "start_time": "2024-10-03T17:34:39.316597Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The provided text is a snippet from a Substack article about the valuation of companies in the technology sector. The author analyzes the EV/NTM Revenue multiple, comparing it to NTM consensus growth expectations, and categorizes companies based on their projected growth rates.  He notes that companies with higher growth rates generally command higher valuations. \\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d38c40-f4c0-427d-8588-fcf71e08a231",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
